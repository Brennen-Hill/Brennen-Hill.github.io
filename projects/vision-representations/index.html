<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Vision-Language Representations | Brennen A. Hill </title> <meta name="author" content="Brennen A. Hill"> <meta name="description" content="Representation Fine-Tuning for Vision-Language Models."> <meta name="keywords" content="Brennen Hill, Brennen A. Hill, Brennen Alexander Hill, Researcher, Neuroscience, AI, NeuroAI, Machine Learning, Reinforcement Learning, World Models, Neuromorphic Computing, Engineer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Favicon.png?bfe1cbd95a5c0b203535788fc4fe0d0e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brennen-hill.github.io/projects/vision-representations/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Brennen</span> A. Hill </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/presentations/">presentations </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header mb-5"> <h1 class="post-title">Vision-Language Representations</h1> <p class="post-description mb-2">Representation Fine-Tuning for Vision-Language Models.</p> <div class="small text-muted"> <p class="d-flex align-items-center mb-1"><i class="fas fa-user-tie fa-fw me-2"></i>Researcher</p> <p class="d-flex align-items-center mb-1"> <i class="fas fa-calendar-alt fa-fw me-2"></i>September 2024 - December 2024</p> <p class="d-flex align-items-center mb-0"><i class="fas fa-users fa-fw me-2"></i> Brennen Hill, Albert Ge, Chapin Pyne, Arnav Sharma, Karl Vachuska</p> </div> </header> <article> <h2 id="abstract">Abstract</h2> <p>While finetuning large vision-language models has proven effective for various downstream tasks, it often requires modifying a substantial portion of model parameters. In this work, we investigate representation finetuning (ReFT) as a parameter-efficient alternative to traditional finetuning approaches. Using spatial reasoning as a benchmark task, we demonstrate that ReFT can match the performance of conventional finetuning methods while reducing the number of tunable parameters by an order of magnitude. Through comparative experiments with baseline models like nanoLlaVA, we show that our ReFT approach achieves comparable accuracy on spatial understanding tasks despite its significantly smaller parameter footprint. Our findings suggest that targeted representation finetuning offers a promising direction for efficient model adaptation, potentially enabling more resource-conscious approaches to vision-language model specialization.</p> <hr> <h2 id="1-introduction">1. Introduction</h2> <p>The rapid advancement of large pre-trained models has revolutionized machine learning, yet adapting these models to downstream tasks often requires substantial compute resources. Parameter-efficient fine-tuning seeks to mitigate this problem by introducing a limited set of tunable parameters while keeping the base model frozen. Methods like low-rank adaption (missing reference) and prompt tuning (missing reference) have shown promise in reducing the computational overhead of model adaptation, but they still require careful design choices about where and how to insert these trainable components.</p> <p>Recently, representation fine-tuning (missing reference) has been introduced as a method of adapting pre-trained models by modifying their internal representations rather than directly altering their existing parameters. Traditional fine-tuning approaches often proceed by freezing some layers of a model while continuing to update the weights of other layers. Although this method can sometimes yield performance gains, it is frequently inefficient and may require extensive computational resources, as the underlying model weights are re-trained. In contrast, representation fine-tuning proposes an alternative paradigm. Instead of modifying the original model parameters, certain nodes or layers, often referred to as “representation points” within the model’s architecture, are identified, and a function is inserted at these points. This function takes as input the original hidden representation at that node and outputs a new transformed representation. Crucially, the parameters of this newly introduced function are trained, while the original model weights remain unchanged. This decouples representation adaptation from direct weight updates, potentially leading to more efficient and targeted improvements.</p> <p>Such a technique has already shown promise in the context of large language models (missing reference), where researchers demonstrated that representation fine-tuning can guide models to better capture certain desired attributes, improve interpretability, and potentially address emergent behaviors. Despite these advances, the application of representation fine-tuning to vision-language models, complex architectures that fuse visual and linguistic inputs, has not yet been fully explored. Introducing representation fine-tuning to this domain could enable more granular control over how these models represent and integrate multimodal data, potentially leading to improvements in tasks such as image captioning, visual question answering, and multimodal reasoning.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vis-reft/spacial_perception-480.webp 480w,/assets/img/vis-reft/spacial_perception-800.webp 800w,/assets/img/vis-reft/spacial_perception-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vis-reft/spacial_perception.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Comparison of spatial depth perception before and after finetuning. nanoLlaVA out of the box answers this question incorrectly, but nanoLlaVA + ReFT produces the correct answer. </div> <p>In this paper, we apply the ReFT framework to multimodal architectures, demonstrating its effectiveness beyond language-only models. In Figure 1, we illustrate the result of ReFT tuning a vision-language model on CV-Bench (missing reference), a spatial reasoning task. Our contributions are summarized as follows:</p> <ul> <li>We provide empirical evidence that representation fine-tuning can achieve performance comparable to traditional fine-tuning approaches while modifying at least an order of magnitude fewer parameters.</li> </ul> <p>Altogether, our work provides a foundation for more resource-efficient adaptation of vision-language models and suggests promising directions for future research in parameter-efficient model tuning.</p> <p>We organize the rest of the paper as follows. Section 2 reviews the related work, situating our approach within the existing literature on parameter-efficient fine-tuning and representation engineering. Section 3 details our technical approach, including the experimental framework and model configurations. In Section 4, we present the results of our experiments, comparing ReFT with LoRA and full fine-tuning in terms of both performance and parameter efficiency. Section 5 discusses the implications of our findings, highlighting key takeaways and directions for future research.</p> <h2 id="2-related-work">2. Related Work</h2> <p>(missing reference) discuss two influential perspectives on understanding cognition in cognitive science. The first, the Sherringtonian view, describes cognition as a direct consequence of node-to-node connections within neural circuits. While this bottom-up perspective has its merits, it struggles to explain complex cognitive phenomena that emerge from intricate patterns of interaction. In contrast, the Hopfieldian view emphasizes cognition as the product of high-dimensional representational spaces, formed from activity patterns across populations of neurons. This top-down view offers a richer framework for explaining higher-level cognitive processes and captures complexities that the Sherringtonian view cannot.</p> <p>These differing perspectives have parallels in machine learning research. Motivated by the Hopfieldian view, various studies focus on understanding or manipulating the representational spaces within artificial neural networks, rather than studying individual neurons or parameters in isolation. Representation engineering, including representation fine-tuning, has gained attention in recent work on large language models (missing reference), aiming to understand and shape their internal representations to achieve desired behaviors.</p> <p>(missing reference), for example, undertakes the ambitious goal of laying the foundations for a ‘cognitive science of AI’. By abstracting away from low-level details such as individual neurons and their synaptic connections,the authors argue for a more conceptual understanding of neural networks, drawing inspiration from how the Hopfieldian perspective interprets biological cognition. They leverage representation engineering to work top-down, analyzing patterns of activation within the model’s latent spaces. By doing so, they show it is possible not only to understand these large language models more deeply but also to modify them in targeted ways, enhancing attributes like honesty. Their findings suggest that by editing internal representations, one can steer model outputs effectively without overhauling the entire model.</p> <p>In (missing reference), the authors introduce a specific representation fine-tuning technique called LoReFT (Low-rank Linear Subspace ReFT). Let us consider a hidden-state representation $h$ within the model. Suppose we have two inputs, $b$ and $ s $, that produce hidden representations $ h_b $ and $ h_s $ at a particular node or layer of the model. Let $ R \in \mathbb{R}^{r \times d} $ be a low-rank projection matrix with orthonormal rows, where $ d $ is the dimensionality of the hidden representation and $ r \leq d $ is the rank of the subspace on which we intervene. We define a linear projection $ W \in \mathbb{R}^{r \times d} $ and a bias vector $ b \in \mathbb{R}^r $. The LoReFT transformation can be expressed as:</p> \[\phi_{\text{LoReFT}}(h) = h + R^T(W h + b - R h).\] <p>Here, the learned parameters $\phi = {R, W, b}\ $ are adjusted during training, while the original parameters of the model remain frozen. This top-down editing of the representation provides a powerful means to guide the model toward new behaviors, aligning its outputs with desired targets by carefully intervening within a specific subspace of its hidden representations.</p> <p>In the realm of vision-language models, related ideas exist but are often still tied to weight modifications. For example, (missing reference) explore editing concepts within diffusion models, a form of generative model that can produce images conditioned on textual prompts. Although the general goal, manipulating internal model concepts, is similar in spirit to representation engineering, their approach differs in one key aspect: rather than leaving model weights intact, they fine-tune these weights to either remove or refine certain concepts.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vis-reft/loReftDiagram-480.webp 480w,/assets/img/vis-reft/loReftDiagram-800.webp 800w,/assets/img/vis-reft/loReftDiagram-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vis-reft/loReftDiagram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Depicted on the left panel is an intervention $ I $, in which the intervention function $ \phi $ is applied to the hidden representations in layer $ l $ at position $ p $. Depicted in the right panel is the intervention function that LoReFT makes use of. It finds an edit vector which modifies the representation (and not the weights) in the linear subspace that is spanned by the rows of R (missing reference). </div> <p>(missing reference) introduces a strategy known as Unified Concept Editing (UCE), enabling the correction of arbitrary issues (such as offensive content, copyright violations, or harmful biases) without requiring additional training data. By providing prompts that indicate which concepts to remove or preserve, UCE can directly manipulate the model’s conceptual space. This approach relies on modifying the model’s weights to permanently remove unwanted concepts, effectively enforcing representational constraints from within.</p> <p>Similarly, (missing reference) focuses on selectively erasing concepts from text-to-image diffusion models. Instead of retraining from scratch, the authors fine-tune weights at crucial points in the model to remove concepts like artistic styles or explicit content. This is achieved by applying fine-tuning to either cross-attention layers (to target concept-specific prompts) or unconditional layers (for global erasure). Through guided negative supervision and adjustments to internal model weights, the model can be nudged away from generating undesired concepts while preserving overall fidelity. This permanent weight-based modification contrasts with representation fine-tuning, which aims to flexibly steer model outputs by adjusting learned functions that operate on existing representations, leaving the original weights untouched.</p> <p>In summary, while there is a growing body of research on modifying internal representations within neural models, both in language and vision domains, current approaches to editing vision-language models often still depend on direct parameter changes. Representation fine-tuning stands as a promising, yet underexplored, alternative for vision-language models. By drawing on the Hopfieldian perspective and leveraging top-down editing of latent representations, it may be possible to create more efficient, interpretable, and controllable vision-language architectures without the computational overhead and risk of catastrophic forgetting that can accompany traditional fine-tuning methods.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vis-reft/ft_methods-480.webp 480w,/assets/img/vis-reft/ft_methods-800.webp 800w,/assets/img/vis-reft/ft_methods-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vis-reft/ft_methods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 3" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Comparing different fine-tuning methods. Full-finetuning and low-rank adapation (LoRA) both make use of gradient-based updates to modify the parameters. Representation finetuning, on the other hand, is applied to the activations at each layer in the model. </div> <p>Figure 3 presents an overview of different fine-tuning approaches. Full fine-tuning (left panel) involves editing all node weights. LoRA (middle panel) involves editing a subset of node weights in an efficient manner uses low-rank adaptations of specific layers. Distinctly, ReFT (right panel) does not involving editing node weights, but editing the representations themselves at certain layers.</p> <h2 id="3-technical-approach">3. Technical approach</h2> <p>In this study, we investigate fine-tuning and generation techniques for large language models with efficient memory utilization using quantized approaches. The methodology integrates state-of-the-art frameworks to address computational constraints while ensuring high-quality model outputs.</p> <h3 id="experimental-framework">Experimental Framework</h3> <p>Our experiments were conducted using the Hugging Face Transformers library, employing a pre-trained model nanoLLaVA <a href="[https://huggingface.co/qnguyen3/nanoLLaVA](https://huggingface.co/qnguyen3/nanoLLaVA)">1</a>, which uses the Quyen-SE <a href="[https://huggingface.co/vilm/Quyen-SE-v0.1](https://huggingface.co/vilm/Quyen-SE-v0.1)">2</a> and SigLIP as the vision encoder (missing reference).</p> <p>The model was fine-tuned on a CUDA-enabled environment, ensuring optimal utilization of available hardware resources. We implemented the experiments using Python 3.8 and PyTorch, leveraging the high compatibility of these frameworks with deep learning workflows.</p> <h3 id="tokenization-and-input-processing">Tokenization and Input Processing</h3> <p>The tokenizer was initialized using the same pre-trained model checkpoint to ensure compatibility. The maximum sequence length was set to 2048 tokens, allowing the model to process extended contexts. A custom template was defined for user-assistant interactions:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;|user|&gt;: {User Prompt}
&lt;|assistant|&gt;: {Assistant Response}
</code></pre></div></div> <p>This template provided a structured input format, ensuring uniformity across training and inference phases.</p> <h3 id="fine-tuning-and-intervention">Fine-Tuning and Intervention</h3> <p>We developed a custom generation function, <code class="language-plaintext highlighter-rouge">generate</code>, to enable fine-grained control over the text generation process. The function facilitates:</p> <ol> <li> <strong>Prompt Interventions:</strong> Allowing modifications to the input prompt during inference.</li> <li> <strong>Source Representations:</strong> Supporting the inclusion of external sources as auxiliary input.</li> </ol> <p>Future iterations of this function aim to incorporate interventions directly during the generation process to provide more dynamic control over model behavior.</p> <h3 id="training-environment">Training Environment</h3> <p>The fine-tuning process was executed on a GPU-accelerated system using NVIDIA’s L4 architecture. All experiments utilized the PyReft library for facilitating training adjustments, alongside Accelerate and PEFT frameworks to streamline distributed training and parameter-efficient fine-tuning.</p> <h3 id="31-training-configurations">3.1. Training Configurations</h3> <p>We conducted experiments using three different fine-tuning approaches: ReFT, LoRA, and full fine-tuning. All methods shared several base hyperparameters, including a learning rate of 1e-3, single epoch training, batch size of 1, and AdamW with bfloat16 mixed precision training.</p> <table> <thead> <tr> <th style="text-align: left"><strong>Param</strong></th> <th style="text-align: left"><strong>Value</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Rank</td> <td style="text-align: left">4</td> </tr> <tr> <td style="text-align: left">Intervene Layers</td> <td style="text-align: left">All</td> </tr> <tr> <td style="text-align: left">Intervene Positions</td> <td style="text-align: left">Start/Mid/End</td> </tr> </tbody> </table> <p>Table 1. ReFT hyperparameters configuration.</p> <table> <thead> <tr> <th style="text-align: left"><strong>LoRA Hyperparameter</strong></th> <th style="text-align: left"><strong>Value</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Rank dimension (r)</td> <td style="text-align: left">8</td> </tr> <tr> <td style="text-align: left">Alpha scaling</td> <td style="text-align: left">16</td> </tr> <tr> <td style="text-align: left">Target modules</td> <td style="text-align: left">All attention layers</td> </tr> </tbody> </table> <p>Table 2. LoRA-specific hyperparameters used in our experiments.</p> <table> <thead> <tr> <th style="text-align: left"><strong>Full Fine-tuning Parameter</strong></th> <th style="text-align: left"><strong>Value</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Trainable parameters</td> <td style="text-align: left">All model parameters</td> </tr> <tr> <td style="text-align: left">Gradient checkpointing</td> <td style="text-align: left">Enabled</td> </tr> <tr> <td style="text-align: left">Weight decay</td> <td style="text-align: left">0.01</td> </tr> </tbody> </table> <p>Table 3. Full fine-tuning specific configuration.</p> <p>ReFT (Table 1) was implemented with a smaller rank dimension of 4, as our preliminary experiments showed this was sufficient for good performance. We applied interventions across all layers of the model, with intervention positions strategically placed at the start, middle, and end of each prompt. This positioning allows ReFT to capture contextual information at critical points in the input sequence. We used a single intervention per position to maintain computational efficiency while still achieving effective adaptation.</p> <p>For LoRA (Table 2), we configured the adaptation with a rank dimension of 8 and scaling factor $ \alpha=16 $. These LoRA adaptations were applied to all attention layers in the model, providing a good balance between parameter efficiency and model capacity. This configuration resulted in significantly fewer trainable parameters compared to full fine-tuning while maintaining strong performance.</p> <p>For the full fine-tuning baseline (Table 3), we updated all model parameters during training. To manage the memory requirements of training the full model, we enabled gradient checkpointing. We applied a weight decay of 0.01 to help prevent overfitting, though as our results show, this approach still performed worse than the parameter-efficient methods despite having access to all model parameters.</p> <table> <thead> <tr> <th style="text-align: left"><strong>Method</strong></th> <th style="text-align: left"><strong>Trainable Parameters (%)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">ReFT</td> <td style="text-align: left">0.019</td> </tr> <tr> <td style="text-align: left">LoRA</td> <td style="text-align: left">0.17</td> </tr> <tr> <td style="text-align: left">Full Fine-tuning</td> <td style="text-align: left">60.99</td> </tr> </tbody> </table> <p>Table 4. Parameter efficiency comparison across fine-tuning methods. Both ReFT and LoRA achieve strong performance while training a tiny fraction of the model’s parameters compared to full fine-tuning.</p> <p>Table 4 highlights the stark differences in parameter efficiency across the three fine-tuning methods. ReFT demonstrates exceptional parameter efficiency, requiring only 0.019% of the model’s parameters to be trained. This makes it the most parameter-efficient approach in our comparison, using nearly an order of magnitude fewer parameters than LoRA while achieving comparable performance.</p> <p>LoRA, while still highly efficient, trains 0.17% of the model’s parameters. This represents a middle ground in our comparison, requiring more parameters than ReFT but still maintaining a very small footprint compared to full fine-tuning.</p> <p>Full fine-tuning updates a substantial 60.99% of the model’s parameters, making it significantly more resource-intensive than both parameter-efficient methods. Despite training orders of magnitude more parameters, it achieves lower performance than both ReFT and LoRA, suggesting that more parameters do not necessarily lead to better spatial reasoning.</p> <p>The training data was drawn from the CV-Bench dataset, with an 80-20 split for training and validation sets respectively.</p> <h2 id="4-experimental-results">4. Experimental Results</h2> <table> <thead> <tr> <th style="text-align: left"><strong>Fine-tuning Method</strong></th> <th style="text-align: left"><strong>Validation Accuracy (%)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">ReFT</td> <td style="text-align: left">65.7</td> </tr> <tr> <td style="text-align: left">LoRA</td> <td style="text-align: left">66.0</td> </tr> <tr> <td style="text-align: left">Full Fine-tuning</td> <td style="text-align: left">51.7</td> </tr> </tbody> </table> <p>Table 5. Comparison of different fine-tuning approaches on the CV-Bench dataset. Both parameter-efficient methods (ReFT and LoRA) significantly outperform traditional full fine-tuning.</p> <p>Table 5 presents the validation accuracy achieved by each fine-tuning method on the CV-Bench dataset. Our experiments reveal several interesting findings regarding the effectiveness of different fine-tuning approaches for vision-language models.</p> <p>Parameter-efficient fine-tuning methods demonstrated superior performance, with both ReFT and LoRA achieving approximately 66% accuracy on the validation set. The nearly identical performance between ReFT (65.7%) and LoRA (66.0%) suggests that both methods are equally effective at adapting the model to the multiple-choice visual question-answering task.</p> <p>Surprisingly, full fine-tuning performed significantly worse, achieving only 51.7% accuracy. This substantial performance gap (approximately 14 percentage points) between parameter-efficient methods and full fine-tuning is particularly noteworthy. Given that random chance in a four-choice task would yield 25% accuracy, full fine-tuning performance, while above chance, suggests potential optimization challenges.</p> <p>Several factors might explain the superior performance of parameter-efficient methods:</p> <ol> <li> <p><strong>Catastrophic Forgetting</strong>: Full fine-tuning may be more susceptible to catastrophic forgetting, where the model loses its pre-trained knowledge while adapting to the new task. ReFT and LoRA, by design, modify fewer parameters and may better preserve the model’s pre-trained capabilities.</p> </li> <li> <p><strong>Optimization Stability</strong>: Parameter-efficient methods, by updating only a subset of the model’s parameters, may provide a more stable optimization landscape. This could lead to more reliable convergence during training.</p> </li> <li> <p><strong>Regularization Effect</strong>: The inherent parameter constraints in ReFT and LoRA might serve as an implicit regularization mechanism, preventing overfitting to the training data.</p> </li> </ol> <p>These results suggest that parameter-efficient fine-tuning methods should be preferred over full fine-tuning for adapting vision-language models to specific tasks, offering both better performance and computational efficiency.</p> <h3 id="41-optimizing-reft">4.1. Optimizing ReFT</h3> <p>We also conducted experiments aiming to increase the accuracy of ReFT. We tested both what layers we intervened on and the low rank we projected onto. As before, all experiments shared used learning rate of 1e-3, single epoch training, batch size of 1, and AdamW optimizer.</p> <p>The first experiment compared the impact of different ranks for the low rank projections on accuracy. We compared rank 4 to rank 14 projections. Each layer was intervened on.</p> <p>Figure 4 shows that while some benefit can be gained from correctly choosing the best low rank, most of that benefit can be achieved by using a close-to-best rank. Furthermore, it shows that using a rank that is too high can severely impact performance. It should be noted that while only 1 epoch was used, loss does stagnate in that epoch. More training is unlikely to improve these less low-rank projections.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vis-reft/var_low_rank_proj-480.webp 480w,/assets/img/vis-reft/var_low_rank_proj-800.webp 800w,/assets/img/vis-reft/var_low_rank_proj-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vis-reft/var_low_rank_proj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 4" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Plot showing the final validation and train accuracy when varying the rank of the projection matrix. Lower rank tends to result in better evaluation performance. </div> <p>When we experimented with intervening on different layers, we made the assumption that it was most important to intervene on later layers. This assumption was made because we believed that ReFT would work best with the higher-level concepts that usually appear in deeper layers. We used rank 6 projections based on our previous experiment.</p> <p>We started by including only the last layer (layer 23), and then we iteratively added the layer before it. Figure 5 presents the validation and training accuracy achieved by each intervention strategy.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vis-reft/int_on_different_layers-480.webp 480w,/assets/img/vis-reft/int_on_different_layers-800.webp 800w,/assets/img/vis-reft/int_on_different_layers-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vis-reft/int_on_different_layers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 5" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5. Lowest layer 20 means that layers 20, 21, 22, and 23 were intervened on. </div> <p>Figure 5, contrary to our assumptions, has only minor improvements with the only later interventions. Instead, there is a massive spike in accuracy when layers 15-23 are intervened on. After that, minor but steady improvement appears with each layer intervened on.</p> <p>This seems to indicate that earlier interventions are more effective. Without further research, it can not be ruled out that this is a result of model architecture, dataset, or task being fine-tuned. This does seem to indicate that Reft can achieve its full effectiveness with only limited interventions, which allow for even fewer parameters to be trained.</p> <p>However, with our current understanding of ReFT, we are unable to realize those computation savings as we don’t know how to figure out this hyper-parameter selection without computationally expensive testing. This is also an issue when figuring out which low-rank projection should be used, especially when considering that different layers likely have different optimal ranks for their low-rank projections.</p> <h2 id="5-conclusion">5. Conclusion</h2> <p>Prior research (missing reference) has demonstrated ReFT to be a highly parameter-efficient approach for fine-tuning language models with minimal efficiency-performance trade-off. In this paper, we expand ReFT to vision-language models, finding similarly that it can achieve comparable accuracy on spatial understanding tasks with much better parameter efficiency relative to alternative approaches.</p> <p>While ReFT shows promise as a fine-tuning method, particularly for enhancing spatial understanding in pre-trained vision models, there is still room to improve its performance relative to existing approaches like LoRA. Future work may further improve ReFT performance in vision-language models by incorporating interventions directly during the generation process, providing more dynamic control over model behavior. Future work could also make ReFT even less computationally expensive through increase understanding of where to intervene and how to select the rank of projections.</p> <h2 id="references">References</h2> <p>Barack, David L and John W Krakauer (2021). “Two views on the cognitive brain”. In: <em>Nature Reviews Neuroscience</em> 22.6, pp. 359-371.</p> <p>Gandikota, Rohit, Joanna Materzyńska, et al. (2023). “Erasing Concepts from Diffusion Models”. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp. 2426-2436. URL: <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf" rel="external nofollow noopener" target="_blank">https://openaccess.thecvf.com/content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf</a>.</p> <p>Gandikota, Rohit, Hadas Orgad, et al. (2024). “Unified Concept Editing in Diffusion Models”. In: <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>. URL: <a href="https://unified.baulab.info/" rel="external nofollow noopener" target="_blank">https://unified.baulab.info/</a>.</p> <p>Hu, Edward J. et al. (2021). <em>LORA: Low-Rank Adaptation of Large Language Models</em>. arXiv: 2106.09685 [cs.CL]. URL: <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2106.09685</a>.</p> <p>Lester, Brian, Rami Al-Rfou, and Noah Constant (2021). <em>The Power of Scale for Parameter-Efficient Prompt Tuning</em>. arXiv: 2104.08691 [cs.CL]. URL: <a href="https://arxiv.org/abs/2104.08691" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2104.08691</a>.</p> <p>Tong, Shengbang et al. (2024). <em>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</em>. arXiv: 2406.16860 [cs.CV]. URL: <a href="https://arxiv.org/abs/2406.16860" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2406.16860</a>.</p> <p>Wu, Zhengxuan et al. (2024). “ReFT: Representation Finetuning for Language Models”. In: <em>arXiv preprint arXiv:2404.03592</em>. URL: <a href="https://arxiv.org/abs/2404.03592" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2404.03592</a>.</p> <p>Zhai, Xiaohua et al. (2023). <em>Sigmoid Loss for Language Image Pre-Training</em>. arXiv: 2303.15343 [cs.CV].</p> <p>Zou, Andy et al. (2023). “Representation Engineering: A Top-Down Approach to AI Transparency”. In: <em>arXiv preprint arXiv:2310.01405</em>. URL: <a href="https://arxiv.org/abs/2310.01405" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2310.01405</a>.</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Brennen A. Hill. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?60dbf2b58b05c5fffc012a60c81b4b3b"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>