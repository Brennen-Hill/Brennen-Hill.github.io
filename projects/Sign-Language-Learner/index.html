<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sign Language Learner | Brennen A. Hill </title> <meta name="author" content="Brennen A. Hill"> <meta name="description" content="Systematic Analysis of Transfer Learning for Sign Language Recognition"> <meta name="keywords" content="Brennen Hill, Brennen A. Hill, Brennen Alexander Hill, Researcher, Neuroscience, AI, NeuroAI, Machine Learning, Reinforcement Learning, World Models, Neuromorphic Computing, Engineer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Favicon.png?bfe1cbd95a5c0b203535788fc4fe0d0e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brennen-hill.github.io/projects/Sign-Language-Learner/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Brennen</span> A. Hill </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Sign Language Learner</h1> <p class="post-description">Systematic Analysis of Transfer Learning for Sign Language Recognition</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Sign.webp" sizes="95vw"></source> <img src="/assets/img/Sign.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SignLanguageLearner" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An example of gesture recognition for sign language, a key step toward assistive communication technologies, credit to Google. </div> <hr> <h3 id="abstract">ABSTRACT</h3> <p>This project investigates computationally efficient and accurate models for static American Sign Language (ASL) alphabet recognition. Using the Sign Language MNIST dataset, we systematically explored the impact of model architecture, transfer learning strategies, and data augmentation on classification performance. We compare three experimental models based on the ResNet-50 architecture against a 97% accuracy baseline. Our primary contribution is the development of a lightweight, fine-tuned model (Model C) that achieves <strong>99.5% test accuracy</strong>. This model, which utilizes a simplified classification head and single-channel (grayscale) inputs, not only surpasses the baseline but also proves significantly more efficient, reducing training time by over 57% compared to a more complex fine-tuning approach (Model B).</p> <hr> <h3 id="introduction">Introduction</h3> <h4 id="problem-statement">Problem Statement</h4> <p>Effective communication is hindered by barriers between different modalities, such as spoken language and sign language. While millions rely on sign language, a significant portion of the global population does not understand it, creating an information gap. Machine learning models for sign language translation are a foundational component of assistive technologies that aim to bridge this divide. While comprehensive systems must interpret dynamic gestures (Li et al.), a critical first step is the robust and efficient classification of static signs, such as the letters of the alphabet.</p> <h4 id="baseline-and-objectives">Baseline and Objectives</h4> <p>Our work utilizes the <strong>Sign Language MNIST dataset from Kaggle</strong>, a common benchmark for static handshape recognition. The established baseline for this dataset is a custom Convolutional Neural Network (CNN) architecture (Sachinpatil1280) that reports a test accuracy of 97%.</p> <p>Our objective was not merely to surpass this accuracy but to conduct a <strong>systematic analysis of transfer learning strategies</strong>, specifically focusing on the trade-offs between model complexity, computational cost (training time), and classification accuracy.</p> <hr> <h3 id="methodology">Methodology</h3> <h4 id="dataset-and-pre-processing">Dataset and Pre-processing</h4> <p>We used the Sign Language MNIST dataset, which contains 27,455 training and 7,172 test images. Each is a 28x28 grayscale image representing one of 24 static letters of the ASL alphabet; ‘J’ and ‘Z’ are excluded as they require motion.</p> <p>To improve generalization and mitigate overfitting, we applied a standardized data augmentation pipeline using <code class="language-plaintext highlighter-rouge">tf.keras.preprocessing.image.ImageDataGenerator</code>:</p> <ul> <li> <strong>Rotation Range:</strong> 10 degrees</li> <li> <strong>Zoom Range:</strong> 0.1</li> <li> <strong>Width/Height Shift Range:</strong> 0.1</li> <li> <strong>Shear Range:</strong> 0.1</li> <li> <strong>Horizontal Flip:</strong> True</li> </ul> <h4 id="model-architectures-and-experiments">Model Architectures and Experiments</h4> <p>We designed a series of three experiments (Models A, B, and C) all based on the <strong>ResNet-50</strong> architecture, pre-trained on ImageNet.</p> <ul> <li> <strong>Model A (Complex Custom Head):</strong> Our initial approach involved appending a deep custom classification head to the ResNet-50 base, consisting of multiple fully-connected layers (512 neurons) and pooling layers. This was intended to maximize the feature extraction capabilities of the pre-trained base.</li> <li> <strong>Model B (Standard Fine-Tuning):</strong> Based on the results of Model A, we shifted to a standard fine-tuning strategy. We appended two 128-neuron dense layers. Training was conducted for 30 epochs: the first 10 with the ResNet-50 base <strong>frozen</strong> to train the new head, and the subsequent 20 with the base <strong>unfrozen</strong> for end-to-end fine-tuning. This model also used a doubled augmentation range to test robustness.</li> <li> <strong>Model C (Efficiency-Focused):</strong> This model was designed to optimize both accuracy and computational efficiency. We simplified the architecture by: <ol> <li> <strong>Reducing the head</strong> to a single 128-neuron dense layer.</li> <li> <strong>Reducing input data</strong> by modifying the ResNet-50 input layer to accept <strong>single-channel (grayscale)</strong> 28x28 images (padded to 32x32), rather than the standard 3-channel RGB input.</li> <li>Reverting to the original, smaller augmentation parameters.</li> </ol> </li> </ul> <hr> <h3 id="results-and-discussion">Results and Discussion</h3> <p>Our experimental results show a clear progression, culminating in a highly accurate and efficient model.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Key Features</th> <th style="text-align: left">Training Time</th> <th style="text-align: left">Test Accuracy</th> <th style="text-align: left">Key Finding</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Baseline</strong></td> <td style="text-align: left">Simple CNN</td> <td style="text-align: left">Not Reported</td> <td style="text-align: left">97.0%</td> <td style="text-align: left">High initial benchmark.</td> </tr> <tr> <td style="text-align: left"><strong>Model A</strong></td> <td style="text-align: left">ResNet-50 + Deep Custom Head</td> <td style="text-align: left">&gt; 24h (for 2 epochs)</td> <td style="text-align: left">&lt; 50% (est.)</td> <td style="text-align: left"> <strong>Computationally intractable;</strong> overly complex head failed to train effectively.</td> </tr> <tr> <td style="text-align: left"><strong>Model B</strong></td> <td style="text-align: left">ResNet-50 + Freeze/Unfreeze + Heavy Aug.</td> <td style="text-align: left">~7 hours</td> <td style="text-align: left">98.5%</td> <td style="text-align: left">Standard fine-tuning surpasses baseline but is time-intensive.</td> </tr> <tr> <td style="text-align: left"><strong>Model C</strong></td> <td style="text-align: left">ResNet-50 + Lightweight Head + Grayscale</td> <td style="text-align: left"><strong>~3 hours</strong></td> <td style="text-align: left"><strong>99.5%</strong></td> <td style="text-align: left"> <strong>Optimal balance:</strong> Highest accuracy and &gt;50% reduction in training time.</td> </tr> </tbody> </table> <h4 id="analysis">Analysis</h4> <p><strong>Model A’s failure</strong> demonstrated that adding excessive parameters to the classification head was computationally intractable and led to unstable training.</p> <p><strong>Model B</strong> confirmed that a standard freeze-unfreeze transfer learning strategy was effective, successfully surpassing the baseline accuracy. However, its 7-hour training time highlighted a significant cost.</p> <p><strong>Model C yielded the most significant findings.</strong> By simplifying the classification head and, critically, modifying the model to accept single-channel grayscale input, we achieved a <strong>2.5% accuracy improvement over the baseline</strong> and a <strong>1% improvement over Model B</strong>. Most importantly, these gains were coupled with a <strong>&gt;57% reduction in training time</strong> compared to Model B.</p> <p>Furthermore, analysis of Model C’s training curves revealed that validation loss began to increase (overfitting) after just 6 epochs. This insight was crucial: by implementing early stopping and saving the best weights (from epoch 6), we achieved the final 99.5% accuracy, demonstrating the model’s high capacity and the critical need for careful regularization.</p> <hr> <h3 id="conclusion-and-future-work">Conclusion and Future Work</h3> <p>Our systematic experimentation demonstrates that for static sign language recognition, a strategically simplified transfer learning model (Model C) significantly outperforms both a standard CNN baseline and a more complex fine-tuning approach. We achieved 99.5% accuracy while substantially reducing computational cost.</p> <p>The primary limitation of this project is the dataset itself, which is constrained to static, single-letter images under controlled conditions. This work serves as a foundation for future research, which should focus on:</p> <ol> <li> <strong>Dynamic Gesture Recognition:</strong> Expanding the model to handle video input to classify motion-based signs (like ‘J’ and ‘Z’) using datasets like WLASL.</li> <li> <strong>Real-World Robustness:</strong> Testing model performance on “in-the-wild” images with varied lighting, backgrounds, and occlusions.</li> <li> <strong>Model Deployment:</strong> Exploring quantization and pruning to deploy an efficient model on edge devices (e.g., mobile phones) for a real-time assistive application.</li> </ol> <hr> <h3 id="contributors">Contributors</h3> <p>Brennen Hill, Joseph Mostika, Mehul Maheshwari</p> <hr> <h3 id="references">References</h3> <p>[1] Wolmark, M. (2023, September 2). <em>79 hearing loss statistics: How many deaf people in the U.S.?</em>. Golden Steps ABA. <a href="https://www.goldenstepsaba.com/resources/hearing-loss-statistics" rel="external nofollow noopener" target="_blank">https://www.goldenstepsaba.com/resources/hearing-loss-statistics</a> [2] Li, D. (n.d.). <em>Welcome to WLASL homepage</em>. WLASL. <a href="https://dxli94.github.io/WLASL/" rel="external nofollow noopener" target="_blank">https://dxli94.github.io/WLASL/</a> [3] CSC321 tutorial 6: Optimization and Convolutional Neural networks. (n.d.). <a href="https://www.cs.toronto.edu/~lczhang/321/tut/tut06.html" rel="external nofollow noopener" target="_blank">https://www.cs.toronto.edu/~lczhang/321/tut/tut06.html</a> [4] Tecperson. (2017, October 20). <em>Sign language mnist</em>. Kaggle. <a href="https://www.kaggle.com/datasets/datamunge/sign-language-mnist" rel="external nofollow noopener" target="_blank">https://www.kaggle.com/datasets/datamunge/sign-language-mnist</a> [5] (Added) Sachinpatil1280. (2020, May). <em>Sign language-mnist-acc-97%</em>. Kaggle. <a href="https://www.google.com/search?q=https://www.kaggle.com/code/sachinpatil1280/sign-language-mnist-acc-97" rel="external nofollow noopener" target="_blank">https://www.kaggle.com/code/sachinpatil1280/sign-language-mnist-acc-97</a></p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Brennen A. Hill. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>