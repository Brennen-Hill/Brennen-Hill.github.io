<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sign Language Learner | Brennen A. Hill </title> <meta name="author" content="Brennen A. Hill"> <meta name="description" content="Researching methods for translating American Sign Language"> <meta name="keywords" content="Brennen Hill, Brennen A. Hill, Brennen Alexander Hill, Researcher, Neuroscience, AI, NeuroAI, Machine Learning, Reinforcement Learning, World Models, Neuromorphic Computing, Engineer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Favicon.png?bfe1cbd95a5c0b203535788fc4fe0d0e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brennen-hill.github.io/projects/SignLanguageLearner/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Brennen</span> A. Hill </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Sign Language Learner</h1> <p class="post-description">Researching methods for translating American Sign Language</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Sign.webp" sizes="95vw"></source> <img src="/assets/img/Sign.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SignLanguageLearner" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An example of a computer vision model interpretting detecting sign language, credit to Google. </div> <h2 id="abstract">ABSTRACT</h2> <p>We worked to make a model that can translate sign language into text and enable greater communication between people throughout the world. We worked with a Kaggle dataset holding images of letters in sign language that are labeled by letter. Although models have already been built using this dataset, we deepened the effectiveness of existing models. We experimented with various combinations of data augmentation, transfer learning, and model architecture. After using what we learned from our experiments to build a highly effective model, we greatly surpassed the baseline.</p> <h2 id="introduction">Introduction</h2> <p><strong>Problem Statement:</strong> Communication is essential for people around the world to work and share ideas with one another. An issue that gets in the way of this communication is the large number of people who suffer from deafness. Though many deaf people communicate through sign language everyday, the majority of people who are not hearing impaired do not take the time to learn it. We intend to make it easier for all people to communicate by building a sign language translator. Work has already been done to convert sign language to text (Li), and we intend to further that effort.</p> <p><strong>Baseline:</strong> For our baseline we looked extensively at the different models used to categorize hand sign images. We wanted to focus on using convolutional neural networks (CNNs) to categorize our images, and as such, chose to focus on a CNN model made by user sachinpatil1280 which reported a 97% accuracy for their model. This implementation takes advantage of a sequential model with an added dense layer with 128 neurons.</p> <h2 id="data">Data</h2> <h3 id="source">Source</h3> <p>We chose to work with the Sign Language MNIST dataset on kaggle, due to MNIST being a popular benchmark for image based machine learning models. You can find the link <a href="https://www.kaggle.com/datasets/datamunge/sign-language-mnist" rel="external nofollow noopener" target="_blank">here</a>.</p> <h3 id="data-breakdown">Data Breakdown</h3> <p>The dataset format is patterned to match closely with the classic MNIST. There are 24 classes on this test set, as the letters J and Z are not contained in the dataset because they require gesture motion. Further iterations of our model could be used to include these letters given video input. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). There are 27455 training cases and 7172 test cases, with each case consisting of pixel image values between 0-255.</p> <h3 id="pre-processing">Pre-Processing</h3> <p>Pre-processing varied depending on the model that we chose, however for both of our models we used the same data augmentation, a snippet of which is provided below. Model A and C take advantage of the parameters pictured below, whereas model B uses double each value in attempts to broaden the dataset. The pictured values below are our optimized augmentation parameters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># data augmentation
</span><span class="n">datagen</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="nc">ImageDataGenerator</span><span class="p">(</span>
    <span class="n">rotation_range</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">zoom_range</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="c1"># zoom_range=0.2,
</span>    <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">fill_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">nearest</span><span class="sh">'</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="model-c">Model C</h3> <p>For this model, we reduced the image size to 28 by 28, and then added padding for a final size of 32 by 32. Furthermore, we worked with greyscale files, reducing the information that the model had to work with by 2/3rds.</p> <h2 id="tasks-performed">Tasks performed</h2> <h3 id="model-a">Model A</h3> <p>In this architecture of the model, we decided to avoid reinventing the wheel and use a pre-existing model. We used transfer learning to make use of ResNet-50. We replaced the input of ResNet-50 with the image size we wanted to use. We removed the classification layer at the end of ResNet-50. Before adding our own classification layer that would include the number of outputs we wanted for the alphabet, we added additional layers. We added multiple fully connected layers with 512 neurons and pooling layers. Our motivation was that by adding more layers, we could tune the model to predict accurately on our dataset, rather than just imagent.</p> <h3 id="model-b">Model B</h3> <p>This succeeding model takes advantage of freezing and unfreezing of base layers within the ResNet-50 architecture at specific times while limiting the complexity of the model. With a total of 30 epochs, the first 10 were run with the base layers frozen, to capitalize on the advantage of the parameters being pre-trained on ImageNet data. The subsequent 20 layers were run with the base layers unfrozen, with attempts to fine-tune our model on our own ASL images. In addition, two dense layers with 128 neurons were added. Finally, the data augmentation parameters we used to dictate rotation, zoom, blur, and other important elements were doubled in hopes that our model would be more robust with a greater variety of input images.</p> <h3 id="model-c-1">Model C</h3> <p>Similarly to our previous models, we used ResNet50 for transfer learning with this model. Since Model B had already achieved a higher accuracy than our baseline, this model was focused on getting as efficient as possible. To do this, we scaled down the additional output layers to a dense layer with 128 neurons. Furthermore we reduced the amount of data augmentation by half as compared to Model B, and used grayscale images, reducing the image size by 2/3rds.</p> <h2 id="results-and-discussions">Results and Discussions</h2> <h3 id="model-a-1">Model A</h3> <p>This architecture turned out to be efficient. We put a multitude of trainable parameters into the model. After training for a day, the model had finished its second epoch and was highly inaccurate. We realized that adding so many fully connected layers would be unreasonable with our current computing powers. We learned that we would need to put a greater emphasis on efficient model design and moved on to architectures that made use of less trainable parameters.</p> <h3 id="model-b-1">Model B</h3> <p>This model succeeded in obtaining a substantial increase in accuracy, however it came at the cost of efficiency and training time. After 10 epochs, this second model achieved an accuracy of 88%, and after 30 epochs, it achieved an accuracy of 98.5% after 30 epochs, which took about 7 hours to complete. Although this accuracy was a vast improvement, the efficiency still needed to be improved.</p> <h3 id="model-c-2">Model C</h3> <p>This model turned out to be both extremely efficient, and after fine tuning, extremely accurate as well. It took about 3 hours to train the model which was a significant improvement. Furthermore, because we trained this model by saving the best parameter weights as opposed to the final result of the training, we were able to maximize the accuracy of our model to 99.5% accuracy.</p> <p>An important aspect to note about this model is that even though we only trained it for 10 epochs, after 6 epochs the loss on the validation set started skyrocketing while the loss on the training set stayed relatively constant. In other words, this model started overfitting after only 6 epochs.</p> <h2 id="contributors">Contributors</h2> <p>Brennen Hill, Joseph Mostika, Mehul Maheshwari</p> <h2 id="references">References</h2> <p>[1] Wolmark, M. (2023, September 2). 79 hearing loss statistics: How many deaf people in the U.S.?. In-Home &amp; Center-Based ABA - Golden Steps ABATM. https://www.goldenstepsaba.com/resources/hearing-loss-statistics</p> <p>[2] Li, D. (n.d.). Welcome to WLASL homepage. WLASL. https://dxli94.github.io/WLASL/</p> <p>[3] CSC321 tutorial 6: Optimization and Convolutional Neural networks. tut06. (n.d.). https://www.cs.toronto.edu/~lczhang/321/tut/tut06.html</p> <p>[4] Tecperson. (2017, October 20). Sign language mnist. Kaggle. https://www.kaggle.com/datasets/datamunge/sign-language-mnist</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2025 Brennen A. Hill. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>